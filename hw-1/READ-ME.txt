READ ME
путеводитель по дз
_____________________________________________________________________

Practice-01-0202.zip -- архив проекта (создавался проект maven, на маке)
Practice-01-0202/src/main/java/org/example/ -- тут лежат Client.java, Server.java

local-output -- данные и графики с локальным тестированием, время на клиенте измерялось через System.nanotime

virtual-output -- данные и графики с сервером на виртуальной машине, время на клиенте измерялось через System.currentTimeMillis

в названии файла указаны параметры в порядке n-m-q и состояние флага TCP_NO_DELAY (no_delay -- socket.setTcpNoDelay(true))

добавлены скриншоты из дебага клиента, wireshark (где показано, что клиент передал серверу данные, а сервер ответил временем), вариант запуска сервера на виртуальной машине (скриншот был сделан при запуске на занятии 02.02, но актуальные данные для графиков были получены на повторном подключении к машинам 04.02, поэтому ip адреса там другие)

plot-script.py -- скрипт для построения графиков по csv-файлам (надеюсь, не страшно, что графики представлены в png картинках)

_____________________________________________________________________

выводы по графикам:

1) заметно высокое среднее время в начале нескольких графиков, когда число отправляемых байт мало: это из-за накладных расходов при соединении, если я правильно помню, при использовании протокола TCP

2) одной из причин выбросов может оказаться слабое вай-фай соединение (диапазон 2,4 ГГц; скорость линии 57/57 mbps), причём в доме где-то 5-7 устройств используют эту сеть, что может привести к снижению производительности

3) n = 1, m = 200, q = 1
отправляем по одному пакету за subiteration, макс размер байт = 1 * 199 + 8 = 207, диапазон наносекунд: [0.0;3.5 * 1e6)
при setTcpNoDelay(false) наблюдается более стабильная работа сети: там выбросы с задержкой до 3e6 наносекунд при размере байт 177 и 198 (приблизительно), а основное время на графике от 0.0 до 3e5 наносекунд
полагаю, из-за того, что размеры пакетов и так маленькие (до 207 байт), а размер буфера на сервере 7000 байт, поэтому при разбиении пакетов на ещё меньшие части оставляет больше накладных расходов, чем пользы
при запуске сервера на виртуальной машине диапазон увеличился: [0;100-120] (уже в миллисекундах), флаг не сильно меняет стабильность сети -- в обоих случаях есть выбросы, а стабильно сеть держится в диапазоне от 10 до 30 миллисекунд, на передачу данных и ожидание ответа требуется больше времени, т.к. сервер уже не в локальной сети

4) n = 32, m = 145, q = 10
макс размер байт = 32 * 144 + 8 = 4616, диапазон наносекунд: [0.0;2.7 * 1e6)
разница между флагами была бы не так заметна, если бы не случайные выбросы, а так в обоих случаях стабильная сеть при [1,5 * 1e5;4e5], в случае setTcpNoDelay(true) график более хаотичный, т.к. верхняя граница там 5.5 * 1e5 (а без флага -- 2.7 * 1e6 из-за выбросов)
с сервером на виртуалке похожая ситуация -- больше выбросов при setTcpNoDelay(true) и стабильное соединение в [10;30] (ms)
возможно, что при setTcpNoDelay(true) выбросы связаны с процессом объединения пакетов

5) n = 64, m = 100, q = 25
макс размер байт = 64 * 99 + 8 = 6344, диапазон наносекунд: [0.0;4 * 1e6)
одинокий выброс аж до 4e6 ns, поломавший стабильность в [1e5;4e5] (ns) при setTcpNoDelay(true), при выключенном -- выбросы поменьше
с сервером на виртуалке -- всё ещё самое стабильное в [10;30] (ms), финальный выброс при dataSize > 6000 при setTcpNoDelay(false) можно объяснить тем, что отправляется несколько довольно больших пакетов без разбиения, а макс размер буфера на сервере = 7000, так что данные занимают практически весь буфер (и отправляемые данные должны ждать освобождения буфера перед следующий отсылкой)


__________________________________________________


AFTER FIXES

архив с проектом обновлен

#new-local-output-after-fix -- новые данные и графики, успела протестировать только на локальной машине

Сделала структуру сообщения Message, в которой хранится длина сообщения и само сообщение

Увеличила размер сообщения в 100 раз, чтобы на графике было чуть очевиднее, что время растет с увеличением размера отправляемых данных

для n = 1 это оказалось не сильно заметно из-за низкого прироста размера отправляемых байт







